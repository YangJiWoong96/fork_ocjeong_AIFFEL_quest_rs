{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804f6593",
   "metadata": {},
   "source": [
    "# miniBERT pretraining with Korean Wiki corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1feb9b4",
   "metadata": {},
   "source": [
    "__할 일__\n",
    "  - MLM, NSP 태스크 예제 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9712e7",
   "metadata": {},
   "source": [
    "__한 일__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723be2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "# import shutil\n",
    "# import zipfile\n",
    "# import copy\n",
    "# from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e463369",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ~/aiffel/bert_pretrained/models\n",
    "! ln -s ~/data/ko_8000.model ~/aiffel/bert_pretrained/models/ko_8000.model\n",
    "! ln -s ~/data/ko_8000.vocab ~/aiffel/bert_pretrained/models/ko_8000.vocab"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d08017b3",
   "metadata": {},
   "source": [
    "! mkdir -p ~/aiffel/bert_pretrained/data\n",
    "! ln -s ~/data/kowiki.txt ~/aiffel/bert_pretrained/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27169435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrained/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrained/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c7956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list, repl_prob=0.8):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i) \n",
    "        else: # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx.append( [i] )\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = []  # mask 된 값\n",
    "    \n",
    "    try: # 'repl_prob' - 마스크로 지정된 토큰을 실제로 교체하는 비율\n",
    "        assert len(repl_prob)==2 and sum(repl_prob)<=1.0\n",
    "        m_p = repl_prob[0]; o_p = sum(repl_prob)\n",
    "    except TypeError as e:\n",
    "        m_p = min(repl_prob, 1.0); o_p = max(m_p, 0.9)\n",
    "\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%(mask_prob)를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번 토큰뭉치를 더해 15%를 넘으면 다음 뭉치\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < m_p:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < o_p: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9a166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688945d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(doc, n_seq, mask_prob, vocab_list,\n",
    "                              swap_prob=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    max_seq = n_seq - 3 # for [CLS], [SEP], [SEP]\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  \n",
    "            if verbose == True:\n",
    "                print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < swap_prob:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            \n",
    "            trim_tokens(tokens_a, tokens_b, max_seq) # max_seq 보다 큰 경우 길이 조절\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            if verbose == True:\n",
    "                print(\"is_next:\", is_next)\n",
    "                print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "                print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            if verbose == True:\n",
    "                print(\"tokens:\", len(tokens), tokens)\n",
    "                print(\"segment:\", len(segment), segment)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(\n",
    "                                tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "            if verbose == True:\n",
    "                print(\"masked tokens:\", len(tokens), tokens)\n",
    "                print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "                print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            if verbose == True: print()\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be625cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  3957761\n"
     ]
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrained/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "print(\"total: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d96bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15, verbose=False):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc, verbose=False):\n",
    "        instances = create_pretrain_instances(doc, n_seq, mask_prob, vocab_list)\n",
    "        if verbose == True: ## - 추가\n",
    "            print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "            print(instances[0])\n",
    "            print(instances[-1])\n",
    "            print()\n",
    "\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "        if not vocab.is_unknown(id):        \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        \n",
    "                        # out_f 파일에 json 형식으로 쓰기\n",
    "                        save_pretrain_instances(out_f, doc); doc = []\n",
    "                        \n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            \n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                \n",
    "                # out_f 파일에 json 형식으로 쓰기\n",
    "                save_pretrain_instances(out_f, doc); doc = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6d7fc2a",
   "metadata": {},
   "source": [
    "# 10여 분 가량 시간 소요\n",
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrained/data/minibert_pre_train.json'\n",
    "\n",
    "seq_len = 128\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3e5ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8136fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43299f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = seq_len\n",
    "max_seq = n_seq - 3 # [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cff16892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faff5f842c91461a9d2a4f035270576c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '[MASK]', '[MASK]', '[MASK]', '鬼', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.', '[SEP]', '[MASK]', '[MASK]', '▁카', '터', '[MASK]', '▁얼', '▁\"', '지', '미', '\"', '[MASK]', '[MASK]', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '[MASK]', '[MASK]', '[MASK]', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [35, 36, 37, 38, 57, 58, 59, 60, 61, 62, 84, 85, 88, 94, 95, 113, 114, 115], 'mask_label': ['▁195', '3', '년', '▁미국', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁지', '미', '▁제임스', '▁카', '터', '▁3', '9', '번째']}\n",
      "enc_token: [5, 10, 1605, 3599, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 6, 6, 6, 6322, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 593, 21, 5007, 399, 1927, 3607, 6, 6, 6, 6, 6, 6, 103, 4313, 4290, 613, 3638, 3718, 98, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 489, 376, 3599, 4, 6, 6, 207, 3714, 6, 1042, 103, 3610, 3686, 3718, 6, 6, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 203, 241, 3602, 1114, 3724, 788, 243, 6, 6, 6, 663, 1647, 3682, 3682, 3625, 203, 3008, 3625, 3616, 16, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  479 3652 3625  243    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  813   17 3599  307  587  931    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   18 3686    0    0 3324    0    0    0    0    0  207 3714    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   49 3632  796    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '[MASK]', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '[MASK]', '[MASK]', '[MASK]', '▁196', '6', '년', '▁조지', '아', '[MASK]', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '[MASK]', '[MASK]', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '[MASK]', '[MASK]', '▁조지', '아', '▁지', '사로', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '[MASK]', '[MASK]', '[MASK]', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '▁1976', '년', '▁대통령', '[MASK]', '[MASK]', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '▁당선', '되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 11, 23, 24, 25, 31, 51, 52, 68, 69, 74, 75, 76, 79, 80, 81, 102, 103], 'mask_label': ['아', '▁그', '▁당선', '되고', ',', '▁주', '▁되', '기', '▁1975', '년까지', '▁근무', '했다', '.', '▁주', '지', '사로', '▁선거', '에']}\n",
      "enc_token: [5, 6, 37, 76, 3667, 2378, 822, 10, 1567, 3668, 3294, 6, 822, 3608, 2386, 2163, 3596, 3671, 969, 213, 3929, 173, 607, 6, 6, 6, 386, 3673, 3625, 1755, 3630, 6, 18, 3620, 822, 3600, 1567, 3668, 1447, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 6, 6, 25, 1755, 3630, 3646, 76, 955, 928, 157, 3821, 61, 3773, 530, 3604, 3372, 523, 6, 6, 1755, 3630, 18, 982, 6, 6, 6, 1755, 3630, 6, 6, 6, 18, 3754, 151, 3604, 243, 3600, 3554, 1733, 3628, 50, 3717, 2046, 114, 3692, 1853, 3599, 4, 3306, 3625, 663, 6, 6, 1114, 3724, 958, 3603, 117, 3674, 54, 75, 4089, 238, 1421, 9, 114, 3692, 3964, 3604, 119, 1486, 807, 2056, 2387, 43, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3630    0    0    0    0    0    0    0    0    0   13    0    0\n",
      "    0    0    0    0    0    0    0    0    0 2387  317 3604    0    0\n",
      "    0    0    0   37    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  450 3614    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 3409  673\n",
      "    0    0    0    0 2711   31 3599    0    0   37 3610  982    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  822 3600    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '▁메', '나', '헴', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁그러나', '▁이것은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁미국의', '▁유대', '인', '斬', '冕', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '[MASK]', '[MASK]', '▁제', '2', '차', '[MASK]', '▁무', '기', '▁제한', '[MASK]', '[MASK]', '▁조', '인', '했다', '.', '[SEP]', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '[MASK]', '[MASK]', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '[MASK]', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [18, 19, 20, 21, 41, 42, 43, 44, 48, 49, 74, 75, 79, 83, 84, 104, 105, 116], 'mask_label': ['▁베', '긴', '▁수상', '과', '▁공', '화', '당', '과', '▁단', '체의', '▁소련', '과', '▁전략', '▁협', '상에', '▁국민', '들의', '▁이후']}\n",
      "enc_token: [5, 3604, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 663, 3644, 334, 3637, 5887, 6, 6, 6, 6, 280, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 330, 1487, 6, 6, 6, 6, 679, 2670, 3628, 7504, 7871, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 456, 3928, 3708, 10, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 6, 6, 30, 3619, 3751, 6, 107, 3614, 1956, 6, 6, 53, 3628, 31, 3599, 4, 207, 3714, 3602, 1921, 596, 1840, 316, 410, 50, 42, 3830, 81, 3713, 137, 6, 6, 42, 917, 18, 3793, 3614, 231, 3375, 530, 3604, 2659, 6, 785, 874, 75, 4089, 3642, 1233, 114, 3692, 1853, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  271 4099 1011 3644    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0   41\n",
      " 3683 3724 3644    0    0    0  164 1314    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0 1302 3644    0    0    0 2835    0    0    0  617\n",
      " 1824    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  968  247    0    0    0    0    0    0\n",
      "    0    0    0    0  165    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁반', '대에', '▁부', '딪', '혀', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁완', '전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '[MASK]', '[MASK]', '[MASK]', '▁또한', '▁박', '정', '희', '▁정', '권의', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '꾼', '▁기념', '▁다', '소', '▁회복', '되었다', '.', '[SEP]', '[MASK]', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '[MASK]', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '[MASK]', '[MASK]', '▁실패', '했다', '.', 'え', '[MASK]', '[MASK]', '▁말', '기에', '[MASK]', '[MASK]', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 22, 23, 24, 57, 58, 65, 84, 100, 101, 105, 106, 107, 110, 111], 'mask_label': ['▁주', '한', '미', '군은', '▁그', '쳤다', '.', '▁관계', '가', '▁그러나', '▁대통령', '▁재', '선에', '▁또한', '▁임', '기', '▁터', '진']}\n",
      "enc_token: [5, 141, 867, 51, 5148, 4178, 6, 6, 6, 6, 443, 3640, 3917, 3636, 1083, 125, 847, 859, 209, 3909, 38, 189, 6, 6, 6, 276, 338, 3642, 4055, 36, 2649, 42, 3830, 550, 50, 786, 2408, 9, 128, 3993, 3683, 969, 3596, 4121, 191, 3604, 2995, 3625, 125, 3662, 27, 3946, 3604, 410, 3607, 2017, 54, 4615, 1600, 29, 3688, 3332, 43, 3599, 4, 6, 37, 3290, 243, 2630, 3708, 42, 3892, 636, 10, 42, 3892, 73, 3771, 1579, 3624, 1827, 1640, 3625, 6, 822, 10, 41, 3683, 1547, 194, 4044, 3681, 1169, 3803, 958, 113, 3596, 3944, 875, 6, 6, 1579, 31, 3599, 6180, 6, 6, 150, 329, 6, 6, 1302, 3601, 26, 2986, 3733, 1323, 3232, 636, 9, 751, 1640, 3625, 2219, 779, 3600, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0   37 3612 3686  941    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0   13 1523 3599    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  704 3608    0    0    0    0    0    0  330    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  663    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  174 2087    0    0    0  276  273 3614    0    0  870 3713\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁밴', '스', '▁국', '무', '장', '관을', '▁조', '문', '사', '절', '로', '▁파견', '했다', '.', '[MASK]', '[MASK]', '[MASK]', '▁군사', '[MASK]', '[MASK]', '▁5', '.', '17', '▁쿠', '데', '타', '에', '▁대해', '▁초기', '에는', '▁강', '하게', '[MASK]', '[MASK]', '[MASK]', '▁미국', '[MASK]', '[MASK]', '▁신', '군', '부를', '▁설', '득', '하는데', ',', '▁한', '계가', '▁있었고', '▁결국', '▁', '묵', '인', '하는', '▁', '듯', '한', '▁태', '도를', '▁보이', '게', '▁', '됐다', '.', '[SEP]', '▁퇴', '임', '▁이후', '▁민간', '▁자', '원을', '▁적극', '▁활용', '한', '썹', '숙', '촐', '▁기', '구', '인', '▁카', '터', '▁재', '단을', '▁설립', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '[MASK]', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '[MASK]', '▁및', '▁기', '니', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 17, 19, 20, 33, 34, 35, 37, 38, 74, 75, 76, 92, 100, 116, 117, 118], 'mask_label': ['▁12', '·', '12', '▁반란', '과', '▁비난', '했으나', ',', '▁정부', '가', '▁비', '영', '리', '▁위해', '▁활동', '▁방', '재', '를']}\n",
      "enc_token: [5, 1228, 3626, 79, 3725, 3651, 1657, 53, 3697, 3620, 3931, 3603, 2338, 31, 3599, 6, 6, 6, 1250, 6, 6, 94, 3599, 1695, 927, 3736, 3732, 3600, 433, 1348, 66, 139, 173, 6, 6, 6, 243, 6, 6, 90, 3722, 1191, 178, 4059, 1294, 3604, 34, 2681, 2492, 875, 3596, 4502, 3628, 38, 3596, 4360, 3612, 227, 701, 3052, 3669, 3596, 1027, 3599, 4, 1382, 3773, 165, 3174, 40, 928, 2929, 2523, 3612, 6187, 4143, 5670, 24, 3653, 3628, 207, 3714, 174, 1574, 686, 3612, 339, 1114, 238, 158, 3756, 3607, 6, 30, 49, 3692, 1654, 822, 209, 3623, 6, 228, 24, 3733, 813, 3740, 3600, 1332, 311, 3635, 4956, 3937, 3699, 3626, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 679, 1412, 4234, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  196 3873 1335    0 2342 3644    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 3560 1003 3604    0  513 3608    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0   77 3715 3622    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  231    0    0    0    0    0\n",
      "    0    0  375    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0   95 3729 3624    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '▁인물', '▁및', '[MASK]', '[MASK]', '▁직접', '▁만나', '▁분', '쟁', '의', '▁원', '인을', '▁근', '본', '적으로', '▁해결', '하기', '▁위해', '▁힘', '썼', '다', '.', '▁이', '▁과정에서', '▁미국', '[MASK]', '[MASK]', '▁갈', '등', '을', '▁보', '이기도', '▁했지만', ',', '▁전', '직', '▁대통령', '의', '▁권', '한', '과', '▁재', '야', '▁유명', '▁인사', '들의', '▁컴', '냥', '▁해결', '해', '▁나', '갔다', '.', '[SEP]', '▁카', '터', '는', '▁카', '터', '▁행정', '부', '[MASK]', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '[MASK]', '[MASK]', '▁전쟁', '과', '▁같이', '▁미국', '이', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁최', '후', '로', '▁선택', '하는', '▁전통', '적', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '▁선', '행', '하는', '▁행', '위에', '▁대해', '▁깊', '은', '▁유', '감을', '▁표시', '[MASK]', '▁미국의', '▁군사', '적', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [14, 15, 36, 37, 49, 50, 51, 57, 58, 72, 85, 86, 92, 93, 94, 95, 122, 126], 'mask_label': ['▁단', '체를', '▁행정', '부와', '▁권', '한', '과', '▁활약', '으로', '▁이후', '▁이', '라크', '▁군사', '적', '▁행', '동을', '▁하며', '▁활동']}\n",
      "enc_token: [5, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 1178, 228, 6, 6, 1069, 2142, 147, 3972, 3601, 129, 1171, 387, 3759, 127, 2317, 167, 231, 947, 4437, 3598, 3599, 8, 2208, 243, 6, 6, 742, 3709, 3607, 47, 1304, 3379, 3604, 25, 3802, 663, 3601, 476, 3612, 3644, 174, 3775, 939, 3329, 247, 993, 4465, 2317, 3645, 58, 1133, 3599, 4, 207, 3714, 3602, 207, 3714, 895, 3638, 6, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 6, 6, 506, 3644, 733, 243, 3597, 6, 6, 6, 6, 130, 3706, 3603, 1715, 38, 1306, 3657, 1646, 3624, 407, 999, 1250, 3657, 236, 1629, 57, 3752, 38, 236, 1157, 433, 1910, 3613, 46, 2196, 2466, 6, 679, 1250, 3657, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  164 1396    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  895 2576    0    0    0    0\n",
      "    0    0    0    0    0    0    0  476 3612 3644    0    0    0    0\n",
      "    0 1102    9    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  165    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    8 3553    0    0    0    0    0 1250 3657  236 1629    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 1368    0    0    0\n",
      "  375    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_43/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_43/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ed1925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int32, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d332a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d965b5a6ddb401f84640e23c8a87251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_43/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_43/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(\n",
    "                        vocab, pretrain_json_path, seq_len, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6718dbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   10, 1605, 3599, 1755, 3630,   41, 3644,  830, 3624, 1135,\n",
       "           52, 3599,   13,   81,   87, 1501, 2247,   25, 3779, 3873, 3667,\n",
       "         3631, 3813, 3873, 4196, 3636, 3779, 3601,  249, 3725, 1232,   33,\n",
       "           52, 3599,    6,    6,    6, 6322, 2780,   14, 1509,  168, 3877,\n",
       "          414,  165, 1697, 4290, 3873, 3703, 3683,  593,   21, 5007,  399,\n",
       "         1927, 3607,    6,    6,    6,    6,    6,    6,  103, 4313, 4290,\n",
       "          613, 3638, 3718,   98, 3878, 3656,  256, 2543,  309,  337, 3735,\n",
       "          181, 3616, 3603,  489,  376, 3599,    4,    6,    6,  207, 3714,\n",
       "            6, 1042,  103, 3610, 3686, 3718,    6,    6,   37, 3418,  416,\n",
       "          810, 3666, 3625,  131, 3662,    7, 3629,  203,  241, 3602, 1114,\n",
       "         3724,  788,  243,    6,    6,    6,  663, 1647, 3682, 3682, 3625,\n",
       "          203, 3008, 3625, 3616,   16, 3599,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  479, 3652, 3625,  243,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  813,   17, 3599,  307,  587,  931,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,   18, 3686,    0,    0,\n",
       "         3324,    0,    0,    0,    0,    0,  207, 3714,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   49, 3632,  796,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], \\\n",
    "pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af485c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da772985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9156fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    # shape - (batch, None, n_seq)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ddd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b34d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1275d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a620d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6de4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6de15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        \n",
    "        return attn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8df0a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        \n",
    "        # attn_mask shape - (batch, None, n_seq)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        \n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        # 'Q', 'K', 'V' shape - (bs, n_head, Q_len, d_head)\n",
    "        # 'attn_mask_m' shape - (batch, None, None, n_seq)\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)\n",
    "\n",
    "        # transpose and liner\n",
    "        attn_out = tf.transpose(attn_out, [0, 2, 1, 3])\n",
    "        # 'attn_out' shape - (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, -1, self.n_head*self.d_head])\n",
    "        # 'attn_out' shape - (bs, Q_len, n_head*d_head)      \n",
    "        attn_out = self.W_O(attn_out)\n",
    "        \n",
    "        # 'attn_out' shape - (bs, Q_len, d_model)      \n",
    "        return attn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39d6b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        \n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "948913e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "846e79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model,\n",
    "                                         embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\")\n",
    "                                                   for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None),\n",
    "                                        name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        \n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        \n",
    "        return embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4890b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh,\n",
    "                  kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "    self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax,\n",
    "      name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "  def call(self, inputs):\n",
    "    outputs = self.dense1(inputs)\n",
    "    outputs = self.dense2(outputs)\n",
    "\n",
    "    return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bef16ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef57b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b526df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95715bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        \n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79121e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5c9f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256,\n",
    "                 \"n_head\": 4,\n",
    "                 \"d_head\": 64,\n",
    "                 \"dropout\": 0.1,\n",
    "                 \"d_ff\": 1024,\n",
    "                 \"layernorm_epsilon\": 0.001,\n",
    "                 \"n_layer\": 3,\n",
    "                 \"n_seq\": 256,\n",
    "                 \"n_vocab\": 0,\n",
    "                 \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99ee58e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 25s 11ms/step - loss: 9.7747 - nsp_loss: 0.7198 - mlm_loss: 9.0549 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.5973 - nsp_loss: 0.6165 - mlm_loss: 7.9808 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7de07c482d30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                   optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_history = test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm),\n",
    "                              epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb89c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "003c4e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f65130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 12000\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf853bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 2000\n"
     ]
    }
   ],
   "source": [
    "run_idx = 0\n",
    "\n",
    "pre_train_model.compile( # compile\n",
    "        loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss),\n",
    "        optimizer=optimizer,\n",
    "        metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})\n",
    "\n",
    "tot_hist = {}\n",
    "histories = []\n",
    "run_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec435f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# del vocab\n",
    "# del vocab_list\n",
    "# del test_model\n",
    "\n",
    "enc_tokens, segments = pre_train_inputs\n",
    "labels_nsp, labels_mlm = pre_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b464576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2000/2000 [==============================] - 248s 124ms/step - loss: 10.8538 - nsp_loss: 0.5267 - mlm_loss: 10.3271 - nsp_acc: 0.7692 - mlm_lm_acc: 0.2981\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.29807, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
      "Epoch 2/6\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 11.1292 - nsp_loss: 0.5446 - mlm_loss: 10.5846 - nsp_acc: 0.7474 - mlm_lm_acc: 0.2882\n",
      "\n",
      "Epoch 00002: mlm_lm_acc did not improve from 0.29807\n",
      "Epoch 3/6\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7401 - nsp_loss: 0.5207 - mlm_loss: 10.2194 - nsp_acc: 0.7773 - mlm_lm_acc: 0.3026\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.29807 to 0.30255, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
      "Epoch 4/6\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 10.9538 - nsp_loss: 0.5336 - mlm_loss: 10.4202 - nsp_acc: 0.7618 - mlm_lm_acc: 0.2946\n",
      "\n",
      "Epoch 00004: mlm_lm_acc did not improve from 0.30255\n",
      "Epoch 5/6\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7325 - nsp_loss: 0.5188 - mlm_loss: 10.2137 - nsp_acc: 0.7799 - mlm_lm_acc: 0.3026\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.30255 to 0.30265, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
      "Epoch 6/6\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7364 - nsp_loss: 0.5199 - mlm_loss: 10.2164 - nsp_acc: 0.7780 - mlm_lm_acc: 0.3027\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.30265 to 0.30268, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = f\"{model_dir}/minibert_pre_train_{run_idx:0>3}.hdf5\"\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(\n",
    "                ckpt_path, monitor=\"mlm_lm_acc\", \n",
    "                verbose=1, save_best_only=True, mode=\"max\",\n",
    "                save_freq=\"epoch\", save_weights_only=True)\n",
    "\n",
    "history = pre_train_model.fit( (enc_tokens, segments), (labels_nsp, labels_mlm),\n",
    "                    epochs=epochs, batch_size=batch_size, callbacks=[save_weights] )\n",
    "\n",
    "for k, v in history.history.items():\n",
    "    nv = tot_hist.setdefault(k, list()); nv.extend(v) \n",
    "\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0343e5b6",
   "metadata": {},
   "source": [
    "Epoch 1/2\n",
    "2000/2000 [==============================] - 254s 127ms/step - loss: 17.0801 - nsp_loss: 0.6139 - mlm_loss: 16.4662 - nsp_acc: 0.6359 - mlm_lm_acc: 0.1341\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.13407, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train.hdf5\n",
    "Epoch 2/2\n",
    "2000/2000 [==============================] - 255s 127ms/step - loss: 16.7311 - nsp_loss: 0.6126 - mlm_loss: 16.1185 - nsp_acc: 0.6378 - mlm_lm_acc: 0.1391\n",
    "\n",
    "Epoch 00002: mlm_lm_acc improved from 0.13407 to 0.13911, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train.hdf5\n",
    "\n",
    "Epoch 1/2\n",
    "2000/2000 [==============================] - 245s 123ms/step - loss: 15.5106 - nsp_loss: 0.6076 - mlm_loss: 14.9030 - nsp_acc: 0.6444 - mlm_lm_acc: 0.1597\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.15970, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 2/2\n",
    "2000/2000 [==============================] - 255s 128ms/step - loss: 14.5296 - nsp_loss: 0.6095 - mlm_loss: 13.9202 - nsp_acc: 0.6387 - mlm_lm_acc: 0.1788\n",
    "\n",
    "Epoch 00002: mlm_lm_acc improved from 0.15970 to 0.17876, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "\n",
    "Epoch 1/6\n",
    "2000/2000 [==============================] - 246s 123ms/step - loss: 13.6842 - nsp_loss: 0.6008 - mlm_loss: 13.0833 - nsp_acc: 0.6544 - mlm_lm_acc: 0.1999\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.19990, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 2/6\n",
    "2000/2000 [==============================] - 255s 128ms/step - loss: 13.4569 - nsp_loss: 0.6041 - mlm_loss: 12.8528 - nsp_acc: 0.6479 - mlm_lm_acc: 0.2089\n",
    "\n",
    "Epoch 00002: mlm_lm_acc improved from 0.19990 to 0.20892, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 3/6\n",
    "2000/2000 [==============================] - 255s 128ms/step - loss: 12.9947 - nsp_loss: 0.5953 - mlm_loss: 12.3994 - nsp_acc: 0.6644 - mlm_lm_acc: 0.2217\n",
    "\n",
    "Epoch 00003: mlm_lm_acc improved from 0.20892 to 0.22172, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 4/6\n",
    "2000/2000 [==============================] - 255s 128ms/step - loss: 12.8680 - nsp_loss: 0.5976 - mlm_loss: 12.2704 - nsp_acc: 0.6602 - mlm_lm_acc: 0.2276\n",
    "\n",
    "Epoch 00004: mlm_lm_acc improved from 0.22172 to 0.22762, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 5/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 12.6111 - nsp_loss: 0.5920 - mlm_loss: 12.0191 - nsp_acc: 0.6704 - mlm_lm_acc: 0.2354\n",
    "\n",
    "Epoch 00005: mlm_lm_acc improved from 0.22762 to 0.23545, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "Epoch 6/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 12.4089 - nsp_loss: 0.5894 - mlm_loss: 11.8194 - nsp_acc: 0.6758 - mlm_lm_acc: 0.2433\n",
    "\n",
    "Epoch 00006: mlm_lm_acc improved from 0.23545 to 0.24327, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_trainI000.hdf5\n",
    "\n",
    "Epoch 1/6\n",
    "2000/2000 [==============================] - 247s 123ms/step - loss: 12.3409 - nsp_loss: 0.5887 - mlm_loss: 11.7522 - nsp_acc: 0.6771 - mlm_lm_acc: 0.2456\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.24563, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 2/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 12.0112 - nsp_loss: 0.5789 - mlm_loss: 11.4323 - nsp_acc: 0.6943 - mlm_lm_acc: 0.2568\n",
    "\n",
    "Epoch 00002: mlm_lm_acc improved from 0.24563 to 0.25681, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 3/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 12.1102 - nsp_loss: 0.5856 - mlm_loss: 11.5246 - nsp_acc: 0.6831 - mlm_lm_acc: 0.2539\n",
    "\n",
    "Epoch 00003: mlm_lm_acc did not improve from 0.25681\n",
    "Epoch 4/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.6970 - nsp_loss: 0.5687 - mlm_loss: 11.1284 - nsp_acc: 0.7099 - mlm_lm_acc: 0.2678\n",
    "\n",
    "Epoch 00004: mlm_lm_acc improved from 0.25681 to 0.26780, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 5/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.8772 - nsp_loss: 0.5791 - mlm_loss: 11.2981 - nsp_acc: 0.6952 - mlm_lm_acc: 0.2620\n",
    "\n",
    "Epoch 00005: mlm_lm_acc did not improve from 0.26780\n",
    "Epoch 6/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.4942 - nsp_loss: 0.5604 - mlm_loss: 10.9338 - nsp_acc: 0.7235 - mlm_lm_acc: 0.2750\n",
    "\n",
    "Epoch 00006: mlm_lm_acc improved from 0.26780 to 0.27504, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "\n",
    "Epoch 1/6\n",
    "2000/2000 [==============================] - 254s 127ms/step - loss: 11.6163 - nsp_loss: 0.5696 - mlm_loss: 11.0467 - nsp_acc: 0.7103 - mlm_lm_acc: 0.2711\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.27109, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 2/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.3975 - nsp_loss: 0.5577 - mlm_loss: 10.8398 - nsp_acc: 0.7287 - mlm_lm_acc: 0.2784\n",
    "\n",
    "Epoch 00002: mlm_lm_acc improved from 0.27109 to 0.27840, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 3/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.3485 - nsp_loss: 0.5560 - mlm_loss: 10.7925 - nsp_acc: 0.7313 - mlm_lm_acc: 0.2805\n",
    "\n",
    "Epoch 00003: mlm_lm_acc improved from 0.27840 to 0.28051, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 4/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.3417 - nsp_loss: 0.5560 - mlm_loss: 10.7857 - nsp_acc: 0.7302 - mlm_lm_acc: 0.2807\n",
    "\n",
    "Epoch 00004: mlm_lm_acc improved from 0.28051 to 0.28074, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 5/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.0771 - nsp_loss: 0.5419 - mlm_loss: 10.5353 - nsp_acc: 0.7502 - mlm_lm_acc: 0.2901\n",
    "\n",
    "Epoch 00005: mlm_lm_acc improved from 0.28074 to 0.29015, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 6/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.2611 - nsp_loss: 0.5526 - mlm_loss: 10.7085 - nsp_acc: 0.7363 - mlm_lm_acc: 0.2834\n",
    "\n",
    "Epoch 00006: mlm_lm_acc did not improve from 0.29015\n",
    "\n",
    "Epoch 1/6\n",
    "2000/2000 [==============================] - 248s 124ms/step - loss: 10.8538 - nsp_loss: 0.5267 - mlm_loss: 10.3271 - nsp_acc: 0.7692 - mlm_lm_acc: 0.2981\n",
    "\n",
    "Epoch 00001: mlm_lm_acc improved from -inf to 0.29807, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 2/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 11.1292 - nsp_loss: 0.5446 - mlm_loss: 10.5846 - nsp_acc: 0.7474 - mlm_lm_acc: 0.2882\n",
    "\n",
    "Epoch 00002: mlm_lm_acc did not improve from 0.29807\n",
    "Epoch 3/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7401 - nsp_loss: 0.5207 - mlm_loss: 10.2194 - nsp_acc: 0.7773 - mlm_lm_acc: 0.3026\n",
    "\n",
    "Epoch 00003: mlm_lm_acc improved from 0.29807 to 0.30255, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 4/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 10.9538 - nsp_loss: 0.5336 - mlm_loss: 10.4202 - nsp_acc: 0.7618 - mlm_lm_acc: 0.2946\n",
    "\n",
    "Epoch 00004: mlm_lm_acc did not improve from 0.30255\n",
    "Epoch 5/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7325 - nsp_loss: 0.5188 - mlm_loss: 10.2137 - nsp_acc: 0.7799 - mlm_lm_acc: 0.3026\n",
    "\n",
    "Epoch 00005: mlm_lm_acc improved from 0.30255 to 0.30265, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n",
    "Epoch 6/6\n",
    "2000/2000 [==============================] - 256s 128ms/step - loss: 10.7364 - nsp_loss: 0.5199 - mlm_loss: 10.2164 - nsp_acc: 0.7780 - mlm_lm_acc: 0.3027\n",
    "\n",
    "Epoch 00006: mlm_lm_acc improved from 0.30265 to 0.30268, saving model to /aiffel/aiffel/bert_pretrained/models/minibert_pre_train_001.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5ecb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_264/3527525045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nsp_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nsp_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(tot_hist['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot([l/20. for l in tot_hist['mlm_loss']], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(tot_hist['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(tot_hist['mlm_lm_acc'], 'k--', label='mlm_lm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e40b264b",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df609b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], \\\n",
    "pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "502127af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list must contain integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43/2271528958.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mDecodeIdsWithCheck\u001b[0;34m(self, ids)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_DecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsAsSerializedProtoWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list must contain integers"
     ]
    }
   ],
   "source": [
    "vocab.decode_ids(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ffedf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(pre_train_inputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06ce40f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f86cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
