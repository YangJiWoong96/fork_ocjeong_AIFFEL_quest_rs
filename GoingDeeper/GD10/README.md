# AIFFEL Campus Online Code Peer Review Templete
- 코더 : 정우철
- 리뷰어 : 양지웅


# PRT(Peer Review Template)
- [O]  **1. 주어진 문제를 해결하는 완성된 코드가 제출되었나요?**
    - 문제에서 요구하는 최종 결과물이 첨부되었는지 확인
        - 중요! 해당 조건을 만족하는 부분을 캡쳐해 근거로 첨부
    * 학습 결과 Loss 감소
    ![image](https://github.com/user-attachments/assets/0d2c47e2-0653-4fd4-a81b-f855c6816520)
    * attention matrix 로 알고리즘 내부 구조 확인
    ![image](https://github.com/user-attachments/assets/7de5d50f-502d-4a91-840b-524c469a3c69)


- [O]  **2. 전체 코드에서 가장 핵심적이거나 가장 복잡하고 이해하기 어려운 부분에 작성된 
주석 또는 doc string을 보고 해당 코드가 잘 이해되었나요?**
    - 해당 코드 블럭을 왜 핵심적이라고 생각하는지 확인
    - 해당 코드 블럭에 doc string/annotation이 달려 있는지 확인
    - 해당 코드의 기능, 존재 이유, 작동 원리 등을 기술했는지 확인
    - 주석을 보고 코드 이해가 잘 되었는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
    * 모델의 핵심적인 알고리즘 구조 이해
    ![image](https://github.com/user-attachments/assets/a41e1bc2-e875-4269-b639-35e1e7585eb7)
    * 직접 구성한 모델 구조를 확인
    ![image](https://github.com/user-attachments/assets/48047be7-7045-46b4-9932-92188156ca4d)

        
- [O]  **3. 에러가 난 부분을 디버깅하여 문제를 해결한 기록을 남겼거나
새로운 시도 또는 추가 실험을 수행해봤나요?**
    - 문제 원인 및 해결 과정을 잘 기록하였는지 확인
    - 프로젝트 평가 기준에 더해 추가적으로 수행한 나만의 시도, 
    실험이 기록되어 있는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
    * 전처리 과정에서 SentencePiece의 역할 고찰
    ![image](https://github.com/user-attachments/assets/1caa492b-3447-4d2e-aaa9-1417019daa61)
    * Positional Encoding 차원 변경 확인
    ![image](https://github.com/user-attachments/assets/936530de-8c47-45a3-bd86-9471ad249437)


- [O]  **4. 회고를 잘 작성했나요?**
    - 주어진 문제를 해결하는 완성된 코드 내지 프로젝트 결과물에 대해
    배운점과 아쉬운점, 느낀점 등이 기록되어 있는지 확인
    - 전체 코드 실행 플로우를 그래프로 그려서 이해를 돕고 있는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부
        
- [O]  **5. 코드가 간결하고 효율적인가요?**
    - 파이썬 스타일 가이드 (PEP8) 를 준수하였는지 확인
    - 코드 중복을 최소화하고 범용적으로 사용할 수 있도록 함수화/모듈화했는지 확인
        - 중요! 잘 작성되었다고 생각되는 부분을 캡쳐해 근거로 첨부


# 회고(참고 링크 및 코드 개선)
```
우철님 고생하셨습니다! 단계별로 확인하면서 진행하신게 돋보이는 코드였습니다. 전 하나하나 확인 안하고 넘기는 바람에 모델 파라미터나 가중치를 쌓인 layer들에서 잘 받아내고 있는지도 모르겠네요... 저도 한 번 summary 찍어봐야겠습니다.
또, ?가 UNK로 찍히는 이유에 대해서도 생각을 남겨두셨는데 제 생각엔 데이터 중 의문문이 많이 없기 때문에 vocab 상위 20000개에 포함되지 못해서 UNK 처리한게 아닌가 싶네요
atention layer matrix는 저도 이상하게 찍히고 번역결과도 이상하네요 ㅠ 아무튼 쉽지 않은 task였는데 고생많으셨습니다!!
```
